---
title: "How does public behavior affect the spread of COVID-19? - Initial report"
author: "Quang Vuong"
output: bookdown::pdf_document2
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
```

I am interested in the present dataset because of many reasons. Most importantly, I would like to work in public health in the future, so this project seems like a good starting point. Furthermore, it is shown in the news that there are aspects of public behavior, such as mask-wearing and social distancing, which are more important in controlling the spread of COVID-19 than others, so I wonder if we can quantify this exactly.

## Introduction

We wish to investigate how public behavior affects the spread of COVID-19 within a community. In particular, our goal is to identify which aspects of public behavior, such as visiting restaurants, visiting bars and using public transit, predict case counts. Since many other factors including population demographics, population density and policy decisions also affect the spread of COVID-19, it is best to focus on datasets for a sufficiently small geographical area (like a city or a county) so that the confounding factors unrelated to the study are controlled.

The present dataset, collected from the `{covidcast}` API, contains daily COVID-19 case counts for 88 days from June 5th, 2021 to September 5th, 2021, along with several indicators of public behavior, in Manhanttan, New York. The indicators of public behavior are:

- `distancing` = Percentage of survey respondents reporting that people maintained a distance of at least 6ft (%)
- `public_transit` = Percentage of survey respondents reporting that they used public transit in the last day (%)
- `worked_outside` = Percentage of survey respondents who was indoors (excluding home) in the last day (%)
- `large_events` = Percentage of survey respondents who attended a crowded event in the last day (%)
- `mask_prop` = Percentage of survey respondents who mostly wore a mask outside in the last week (%)
- `other_mask_prop` = Percentage of survey respondents saying that other people mostly wore a mask outside (%)
- `bar_visit` = Number of bar visits per 100000 people (visits per 100000 people)
- `resto_visit` = Number of restaurant visits per 100000 people (visits per 100000 people)

Most of these indicators are obtained from Facebook surveys. Taking into account the issues of survey data such as response bias and subjectivity of answers, it is extremely unlikely that the variables in the dataset track their true respective quantities. Instead, we opt to interpret these indicators as proxies of public behavior, which are potentially useful for prediction.

Now we will summarize our two objectives for this study:
\begin{enumerate}
\item Identify which of the above indicators are important in predicting COVID-19 case counts.
\item Identify a model which can be used to predict future COVID-19 case counts.
\end{enumerate}

## Exploratory data analysis

We first examine the descriptive statistics of the response and all predictors. We present the mean and standard deviation of the predictors and case counts in Table \@ref(tab:desc-stat). The units are as described in the introduction.

```{r desc-stat}
covid <- read.table("pblc_bhv_covid.csv",sep = ',', header=TRUE)
means <- sapply(subset(covid,select=-date), mean)
sds <- sapply(subset(covid,select=-date), sd)
desc <- data.frame(Mean = means, Standard.deviation = sds)

knitr::kable(desc, digits = 2, caption = "Descriptive statistics of predictors and case counts.", format = "pipe")

```

It is clear that all variables have very different scales from each other, so models with unstandardized and standardized variables should be examined. However, these statistics do not inform of the distribution of predictors. To gain information about this, we look at boxplots of the predictors and case counts, as in Figure \@ref(fig:indiv-boxplots). We can see that the distributions of `distancing`, `bar_visit`, `other_mask_prop`, and `cases` are skewed to the right, while the distributions of the others are roughly symmetrical. This concludes our examination of individual variables.

```{r indiv-boxplots, echo=FALSE, fig.height=8, fig.cap = "Boxplots of individual predictors and case counts."}
par(mfrow=c(3,3))
for (i in 2:10) {
  boxplot(covid[i], xlab=names(covid)[i], ylab="Values")
}
```

Now, we will look at how predictors are related to case counts and each other. We plot all predictors against the number of cases in Figure \@ref(fig:plots-against-cases). `distancing`, `bar_visit` and `large_events` appear to have two clusters with different mean case counts. On the other hand, `mask_prop`, `other_mask_prop`, `resto_visit` and `worked_outside` appear to have two trend lines. `public_transit` also looks like there are two trend lines, but both are quite flat. This strongly suggests that there are two clusters within the data that exhibit different relationships between case counts and public behavior.

```{r plots-against-cases, echo=FALSE, fig.height = 8, fig.cap = "Scatterplots of predictors against case counts."}

par(mfrow=c(3,3))

with(covid,plot(distancing,cases))
with(covid,plot(bar_visit,cases))
with(covid,plot(large_events,cases))
with(covid,plot(mask_prop,cases))
with(covid,plot(other_mask_prop,cases))
with(covid,plot(public_transit,cases))
with(covid,plot(resto_visit,cases))
with(covid,plot(worked_outside,cases))
```

To continue the examination of the dataset, we will now look at the covariance matrix of the predictors.

```{r correlations, fig.cap = "Entries of correlation matrix with high coefficients."}
predictors <- subset(covid, select=-c(cases,date))

cor_pred <- cor(as.matrix(predictors))

for (i in 1:ncol(predictors)) {
  print(cor_pred[i, abs(cor_pred[i,]) > 0.6])
}
```

There is some substantial correlation between some predictors. In particular, `bar_visit`, `mask_prop`, `large_events` and `other_mask_prop` seem correlated with each other. Of these predictors, `large_events` appear to have two clusters with different mean case counts, while the remaining predictors seem to have two trend lines against case counts.

The most striking observation made so far is that there might be two clusters within the data that shows different relations between case counts and public behavior indicators, so we will now try to identify how the clusters are split. To do this, we attempt to fit a regression tree and look at the split at the root. Please note that we are only using this tree to expedite the exploration rather than seriously considering as a model. The fitted tree is plotted in Figure \@ref(fig:eda-tree).

```{r eda-tree, echo = F, fig.height=3, fig.cap = "Regression tree to determine clusters in data."}
library(rpart)

covid$time <- 1:nrow(covid)

eda_tree <- rpart(cases ~ .-date, data = covid, method = "anova")
plot(eda_tree, uniform = F, margin = 0.5)
text(eda_tree, pretty = T)
```

Initial inspection of this tree shows that there is clustering by date. The set of plots in Figure \@ref(fig:plot-time-split) corresponds to the first cluster of the dataset when it is split by date. It is clear that splitting by date reduces the clustering behavior so that trends are now much clearer. For completeness, the plots against cases for the remaining cluster are presented in Figure \@ref(fig:plot-time-split2). Interestingly, there does not seem to be trends forming for this cluster.

```{r plot-time-split, echo = F, fig.height=8, fig.cap = "Scatterplots of predictors against case counts on and before July 25th, 2021."}
par(mfrow=c(3,3))

covid_ts1 <- covid[covid$time < 51.5, ]

with(covid_ts1,plot(distancing,cases))
with(covid_ts1,plot(bar_visit,cases))
with(covid_ts1,plot(large_events,cases))
with(covid_ts1,plot(mask_prop,cases))
with(covid_ts1,plot(other_mask_prop,cases))
with(covid_ts1,plot(public_transit,cases))
with(covid_ts1,plot(resto_visit,cases))
with(covid_ts1,plot(worked_outside,cases))

#Determines point in time where data is split.
#covid$date[covid$time == 51]
```

```{r plot-time-split2, echo = F, fig.height=8, fig.cap = "Scatterplots of predictors against case counts after July 25th, 2021."}
par(mfrow=c(3,3))

covid_ts2 <- covid[covid$time >= 51.5, ]

with(covid_ts2,plot(distancing,cases))
with(covid_ts2,plot(bar_visit,cases))
with(covid_ts2,plot(large_events,cases))
with(covid_ts2,plot(mask_prop,cases))
with(covid_ts2,plot(other_mask_prop,cases))
with(covid_ts2,plot(public_transit,cases))
with(covid_ts2,plot(resto_visit,cases))
with(covid_ts2,plot(worked_outside,cases))
```

When checking the correlation matrices within the first cluster, it appears that splitting into clusters has made the correlation between predictors worse. However, the trends are clearer when clusters are made, so we will keep this approach; a regularization method will become useful as a result. The coefficients of `time` in the correlation matrix are large, so it appears that public behavior changes over time, most likely in reaction to the changes in case counts over the months. This is still visible in the correlation matrix of the whole dataset with `time` included, as its coefficients are still relatively high. For this reason, we will analyze the dataset as a whole and in clusters by time.

```{r corr_split, fig.cap = "Entries of correlation matrix with high coefficients for points on and before July 25th 2021.", eval = F}
predictors_ts <- subset(covid_ts, select=-c(cases,date))

cor_pred_ts <- cor(as.matrix(predictors_ts))

for (i in 1:ncol(predictors_ts)) {
  print(cor_pred_ts[i, abs(cor_pred_ts[i,]) > 0.6])
}
```

```{r corr_whole_after_ts, fig.cap = "Entries of correlation matrix with high coefficients with `time` included.", eval = F}
predictors <- subset(covid, select=-c(cases,date))

cor_pred <- cor(as.matrix(predictors))

for (i in 1:ncol(predictors)) {
  print(cor_pred[i, abs(cor_pred[i,]) > 0.6])
}
```

Now, we will outline potential approaches to the analysis of this dataset. As mentioned above, we will analyze the entire dataset as one, and then repeat when the dataset is divided into clusters or with time as an encoded categorical variable instead. Firstly, univariate linear models of each predictor against case counts will be examined to gain an initial understanding of how each predictor affects case counts, and a full linear model will be presented to see how the predictors fit together. Secondly, a regularization approach will be employed due to the correlation of many predictors. Most likely, LASSO will be used in order to enable the ranking of the impact of predictors, as it matches the goal of the analysis. As interpretation is one of the goals of the analysis, we opt to avoid non-parametric approaches. However, repeating the first two approaches with various transformations of the predictors is a possibility; this enables us to accurately model the dataset without sacrificing too much interpretability. Finally, once the main questions have been answered by the above two methods, we will attempt to see if the clusters can be characterized by the predictors instead of by date, which is a classification problem. We will decide the range of approaches to employ for this last component of the analysis once we have settled whether we want to interpret the characterization of the clusters or not.

## Analysis

Models will be selected by leave-one-out CV score using mean squared error.

```{r full}
library(glmnet)
library(tidyverse)
library(MASS)
full_mod <- lm(cases ~ .-date-time, data = covid)
summary(full_mod)

cv_fullmod <- mean(full_mod$residuals^2/(1-diag(hatvalues(full_mod)))^2)
cv_fullmod
```
The first model fitted is a full model that does not split by time. Its CV scores is 3594.374.
```{r full-by-time}
covid <- covid %>% mutate(ts = as.factor(time >= 51.5))

full_mod1 <- lm(cases ~ ts*(.-date-time), data = covid)
summary(full_mod1)

cv_fullmod1 <- mean(full_mod1$residuals^2/(1-diag(hatvalues(full_mod1)))^2)

cv_fullmod1
```
The second model fitted is a full model that fits different models depending on the identified clusters by time. Its CV score is 651.5509, which is much better than the first full model.
```{r lasso}
lasso_mod <- cv.glmnet(x = matrix(unlist(covid[,2:9]), nrow = nrow(covid)),
                       y = covid$cases)

lasso_mod$cvm[lasso_mod$lambda == lasso_mod$lambda.min]

coef(lasso_mod$glmnet.fit, s = lasso_mod$lambda.min)
```

```{r lasso-by-time}
lasso_mod1 <- cv.glmnet(x = matrix(unlist(covid[,c(2:9,12)]), nrow = nrow(covid)),
                       y = covid$cases)

lasso_mod1$cvm[lasso_mod1$lambda == lasso_mod1$lambda.min]

coef(lasso_mod1$glmnet.fit, s = lasso_mod$lambda.min)
```
The next two models are fitted using LASSO. The first of them does not split by time, while the second does. These models have larger CV scores than the unregularized models, indicating that it is likely that all social behavior indicators are important in predicting case counts. Hence it is sensible to attempt ridge regression now.

```{r ridge}
ridge_mod <- cv.glmnet(x = matrix(unlist(covid[,2:9]), nrow = nrow(covid)),
                       y = covid$cases, alpha = 0)

ridge_mod$cvm[ridge_mod$lambda == ridge_mod$lambda.min]

coef(ridge_mod$glmnet.fit, s = ridge_mod$lambda.min)
```

```{r ridge-by-time}
ridge_mod1 <- cv.glmnet(x = matrix(unlist(covid[,2:9,12]), nrow = nrow(covid)),
                       y = covid$cases, alpha = 0)

ridge_mod1$cvm[ridge_mod1$lambda == ridge_mod1$lambda.min]

coef(ridge_mod1$glmnet.fit, s = ridge_mod1$lambda.min)
```
The models fitted by ridge regression performed worse, so the most preferred models are still the full models. We now attempt stepwise variable selection on the full model that recognizes the time split.

```{r stepwise-with-time}
forsel_mod <- stepAIC(full_mod1, list(lower = cases ~ 0, upper = cases ~ ts*(.-date-time-ts)), direction = "both")

summary(forsel_mod)
```
The stepwise selection procedure ended up selecting a model which omits `resto_visit` as well as interaction terms of the time split with `bar_visit`, `large_events` and `worked_outside`. To correctly assess the quality of this model, we must perform a more elaborate CV procedure.
